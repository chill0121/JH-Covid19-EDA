---
title: "Johns Hopkins' Covid-19 Data EDA"
author: "Cody Hill"
date: "2023-04-25"
output:
  pdf_document: default
  html_document: default
---

## Setup
***

Note, before using knitr please install all missing packages from the code cell below into your environment. 

Also, I recommended knitting into HTML as it has been optimized for viewing in that format.

### Data Source Information

**"COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University"**

This data was procured from <https://github.com/CSSEGISandData/COVID-19> which is a github repository where Johns Hopkins compiled covid-19 data. This repository stopped updating on 3/10/23.
In this report we are specifically looking at the `./csse_covid_19_data/csse_covid_19_time_series` data.

I encourage all who reads this report to go read the `readme.md` file in the source's repository to gain more insights about where this data was collected, how it was validated, and compiled into this dataset.
In short, the US data was collected from individual state and county Departments of Health and the global data was collected from various government bodies within each country (with a few exceptions).

*This data set is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) by the Johns Hopkins University on behalf of its Center for Systems Science in Engineering. Copyright Johns Hopkins University 2020.*

#### Feature Descriptions from Source

- **FIPS:** US only. Federal Information Processing Standards code that uniquely identifies counties within the USA.
- **Admin2:** County name. US only.
- **Province_State:** Province, state or dependency name.
- **Country_Region:** Country, region or sovereignty name. The names of locations included on the Website correspond with the official designations used by the U.S. Department of State.
- **Lat and Long_:** Dot locations on the dashboard. All points (except for Australia) shown on the map are based on geographic centroids, and are not representative of a specific address, building or any location at a spatial scale finer than a province/state. Australian dots are located at the centroid of the largest city in each state.
- **Cases:** Counts include confirmed and probable (where reported).
- **Deaths:** Counts include confirmed and probable (where reported).
- **UID:** Unique Identifier for each row entry.
- **ISO3:** Officially assigned country code identifiers.

### Environment Setup

First we will import the libraries in R that are needed.

Then, we will import the data using a URL directly from the source, this means it will update anytime we reknit.

```{r Setup RMD}
# Output all commands run and set a standard plot size
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.height = 6)
# Import Libraries
library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggmap)
library(gridExtra)
# Import dataset
us_cases <- read.csv("https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv")
us_deaths <- read.csv("https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv")
global_cases <- read.csv("https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv")
global_deaths <- read.csv("https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv")
```

## Initial Look at the Data
***

Let's look at the data in its raw form to decide what to do with it.

```{r First Look}
dim(us_cases)
dim(us_deaths)
dim(global_cases)
dim(global_deaths)

head(us_deaths[1:14])
head(global_deaths[1:6])
```

*I'm slicing the output so we don't see the date columns as they will fill the screen.*
Here we can see each date is in an individual column **(feature)**, also `us_deaths` has an extra feature named `Population`.

Also, interesting to see reporting started on January, 22, 2020.

## Clean / Transformation Stage
***

To clean up these datasets we will do the following,

US Data:

- Pivot the date columns into rows
- Merge the cases and deaths datasets, keeping population

Global Data:

- Pivot the date columns into rows
- Merge the cases and death datasets
- Import population data for each country and add a population column

Also in both datasets,

- Remove redundant features and several that we won't be using
- Rename a few features for consistency and readability
- Transform feature class types for easier analysis
- Check for duplicates and missing entries, NA, Null etc.

### Cleaning US Datasets

```{r Cleaning US Data}
# US_Deaths transformations
us_deaths <- us_deaths %>%
    # Pivot all the date columns
    pivot_longer(., -c('UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Province_State', 'Country_Region', 'Lat', 'Long_', 'Combined_Key', 'Population'),
        names_to = 'Date',
        values_to = 'Deaths') %>%
    # Remove unnecessary features
    select(., -iso2, -iso3, -code3, -FIPS, -Combined_Key) %>%
    # Rename some features
    rename(., County = 'Admin2',
        Long = 'Long_')

# US_Cases transformations
us_cases <- us_cases %>%
    # Pivot all the date columns
    pivot_longer(., -c('UID', 'iso2', 'iso3', 'code3', 'FIPS', 'Admin2', 'Province_State', 'Country_Region', 'Lat', 'Long_', 'Combined_Key'),
        names_to = 'Date',
        values_to = 'Cases') %>%
    # Remove unnecessary features
    select(., -iso2, -iso3, -code3, -FIPS, -Combined_Key) %>%
    # Rename some features
    rename(., County = 'Admin2',
        Long = 'Long_')

# Merge into one dataframe
us_data <- full_join(us_cases, us_deaths)

# Remove the X in the dates and mutate to Date class
us_data$Date <- gsub('X', '', as.character(us_data$Date))
us_data <- us_data %>%
    mutate(Date = mdy(Date))

# Change character classes and UID into factors
us_data <- us_data %>%
        mutate(across(where(is.character), as.factor)) %>%
        mutate(UID = as.factor(UID))

# Display changes
head(us_data)
```

This will be much easier to work with.

### Cleaning US Datasets

Now let's do a similar procedure with the global data.

```{r Cleaning Global Data}
# Global_deaths transformations
global_deaths <- global_deaths %>%
    # Pivot all the date columns
    pivot_longer(., -c('Province.State', 'Country.Region', 'Lat', 'Long'),
        names_to = 'Date',
        values_to = 'Deaths') %>%
    # Rename some features
    rename(., Province_State = 'Province.State',
        Country_Region = 'Country.Region')

# Global_Cases transformations
global_cases <- global_cases %>%
    # Pivot all the date columns
    pivot_longer(., -c('Province.State', 'Country.Region', 'Lat', 'Long'),
        names_to = 'Date',
        values_to = 'Cases') %>%
    # Rename some features
    rename(., Province_State = 'Province.State',
        Country_Region = 'Country.Region')

# Merge into one dataframe
global_data <- full_join(global_cases, global_deaths)

# Remove the X in the dates and mutate to Date class
global_data$Date <- gsub('X', '', as.character(global_data$Date))
global_data <- global_data %>%
    mutate(Date = mdy(Date))

# Display changes
head(global_data)
```

Now because we might be interested in making some per capita comparisons in the future, let's import population data and add it to each country in `global_data`.

```{r Import Population to Global}
# Import Table with Populations
UID_Table <- read.csv('https://github.com/CSSEGISandData/COVID-19/raw/master/csse_covid_19_data/UID_ISO_FIPS_LookUp_Table.csv')
head(UID_Table)
# Merge population in using left_join
global_data <- global_data %>%
    left_join(UID_Table, by = c('Province_State', 'Country_Region', 'Lat')) %>%
    select(-c(UID, iso2, iso3, code3, FIPS, Admin2, Combined_Key, Long_))

# Change character classes into factors
global_data <- global_data %>%
        mutate(across(where(is.character), as.factor))
```

Finally, we should take a look at a summary of our two datasets and see if we can make any high level conclusions.

```{r Summary}
summary(us_data)
summary(global_data)
```

Looks like there are a few issues in the US data. The minimum of cases and deaths is showing a negative value.

```{r Negative Cases and Deaths}
filter(us_data, Cases < 0 | Deaths < 0)
```

Looks to be all from the same 3 entries. Not a significant amount of data, easiest to just drop the rows in this case.
Also, since cases and deaths are cumulative sums (rolling total) we aren't missing much if we do drop them (i.e. we will likely pick up whatever these were intended to be in the next valid entry).

```{r Drop Negatives}
# Before
dim(us_data)
us_data <- us_data[us_data$Cases >= 0 | us_data$Deaths >= 0, ]
# After
dim(us_data)
```

We can see here that we've successfully removed the 3 bad entries.